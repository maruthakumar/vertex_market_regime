"""
WebSocket Performance Optimizer for Market Regime Detection System

This module provides comprehensive WebSocket performance optimization to support
50+ concurrent users with <50ms response times and efficient message handling.

Features:
1. Connection pooling and management
2. Message batching and compression
3. Adaptive rate limiting
4. Performance monitoring and metrics
5. Load balancing across multiple workers
6. Memory-efficient data structures
7. Binary protocol support for low latency
8. Intelligent caching and data deduplication

Author: The Augster
Date: 2025-01-16
Version: 1.0.0
"""

import asyncio
import json
import logging
import time
import gzip
import pickle
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Set, Callable
from dataclasses import dataclass, asdict
from collections import deque, defaultdict
from enum import Enum
import weakref
import threading
from concurrent.futures import ThreadPoolExecutor
import websockets
from websockets.server import WebSocketServerProtocol

logger = logging.getLogger(__name__)

class MessageType(Enum):
    """WebSocket message types"""
    TEXT = "text"
    BINARY = "binary"
    COMPRESSED = "compressed"

class CompressionLevel(Enum):
    """Compression levels for messages"""
    NONE = 0
    LOW = 1
    MEDIUM = 6
    HIGH = 9

@dataclass
class PerformanceMetrics:
    """Performance metrics tracking"""
    total_connections: int = 0
    active_connections: int = 0
    messages_sent: int = 0
    messages_received: int = 0
    bytes_sent: int = 0
    bytes_received: int = 0
    average_response_time: float = 0.0
    peak_response_time: float = 0.0
    compression_ratio: float = 0.0
    cache_hit_rate: float = 0.0
    error_count: int = 0
    last_update: datetime = None

@dataclass
class ConnectionStats:
    """Individual connection statistics"""
    connection_id: str
    connected_at: datetime
    last_activity: datetime
    messages_sent: int = 0
    messages_received: int = 0
    bytes_sent: int = 0
    bytes_received: int = 0
    average_latency: float = 0.0
    compression_enabled: bool = False

class WebSocketPerformanceOptimizer:
    """
    WebSocket Performance Optimizer
    
    Provides comprehensive performance optimization for WebSocket connections
    to support 50+ concurrent users with <50ms response times.
    """
    
    def __init__(self, max_connections: int = 100, target_latency_ms: float = 50.0):
        """Initialize WebSocket Performance Optimizer"""
        self.max_connections = max_connections
        self.target_latency_ms = target_latency_ms
        
        # Connection management
        self.connections: Dict[str, WebSocketServerProtocol] = {}
        self.connection_stats: Dict[str, ConnectionStats] = {}
        self.connection_groups: Dict[str, Set[str]] = defaultdict(set)
        
        # Performance tracking
        self.metrics = PerformanceMetrics()
        self.latency_history = deque(maxlen=1000)
        self.throughput_history = deque(maxlen=100)
        
        # Message batching
        self.message_batch_size = 10
        self.batch_timeout_ms = 10  # 10ms batch timeout
        self.pending_batches: Dict[str, List[Dict]] = defaultdict(list)
        self.batch_timers: Dict[str, asyncio.Task] = {}
        
        # Caching and deduplication
        self.message_cache: Dict[str, bytes] = {}
        self.cache_ttl = 60  # 60 seconds
        self.cache_cleanup_interval = 300  # 5 minutes
        self.last_cache_cleanup = time.time()
        
        # Compression settings
        self.compression_threshold = 1024  # Compress messages > 1KB
        self.default_compression_level = CompressionLevel.LOW
        
        # Rate limiting
        self.rate_limit_per_connection = 100  # messages per second
        self.rate_limit_window = 1.0  # 1 second window
        self.rate_limit_counters: Dict[str, deque] = defaultdict(lambda: deque(maxlen=100))
        
        # Thread pool for CPU-intensive operations
        self.thread_pool = ThreadPoolExecutor(max_workers=4)
        
        # Background tasks
        self.cleanup_task = None
        self.metrics_task = None
        self.running = False
        
        logger.info(f"WebSocket Performance Optimizer initialized (max_connections: {max_connections})")
    
    async def start_optimizer(self):
        """Start the performance optimizer"""
        try:
            self.running = True
            
            # Start background tasks
            self.cleanup_task = asyncio.create_task(self._cleanup_loop())
            self.metrics_task = asyncio.create_task(self._metrics_loop())
            
            logger.info("üöÄ WebSocket Performance Optimizer started")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to start optimizer: {e}")
            raise
    
    async def stop_optimizer(self):
        """Stop the performance optimizer"""
        try:
            self.running = False
            
            # Cancel background tasks
            if self.cleanup_task:
                self.cleanup_task.cancel()
            if self.metrics_task:
                self.metrics_task.cancel()
            
            # Cancel pending batch timers
            for timer in self.batch_timers.values():
                timer.cancel()
            
            # Shutdown thread pool
            self.thread_pool.shutdown(wait=True)
            
            logger.info("‚úÖ WebSocket Performance Optimizer stopped")
            
        except Exception as e:
            logger.error(f"‚ùå Error stopping optimizer: {e}")
    
    def register_connection(self, connection_id: str, websocket: WebSocketServerProtocol, 
                          group: str = "default") -> bool:
        """Register a new WebSocket connection"""
        try:
            if len(self.connections) >= self.max_connections:
                logger.warning(f"Connection limit reached: {len(self.connections)}/{self.max_connections}")
                return False
            
            self.connections[connection_id] = websocket
            self.connection_stats[connection_id] = ConnectionStats(
                connection_id=connection_id,
                connected_at=datetime.now(),
                last_activity=datetime.now()
            )
            self.connection_groups[group].add(connection_id)
            
            self.metrics.total_connections += 1
            self.metrics.active_connections = len(self.connections)
            
            logger.debug(f"‚úÖ Connection registered: {connection_id} (group: {group})")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error registering connection {connection_id}: {e}")
            return False
    
    def unregister_connection(self, connection_id: str):
        """Unregister a WebSocket connection"""
        try:
            if connection_id in self.connections:
                del self.connections[connection_id]
            
            if connection_id in self.connection_stats:
                del self.connection_stats[connection_id]
            
            # Remove from all groups
            for group_connections in self.connection_groups.values():
                group_connections.discard(connection_id)
            
            # Cancel pending batches
            if connection_id in self.batch_timers:
                self.batch_timers[connection_id].cancel()
                del self.batch_timers[connection_id]
            
            if connection_id in self.pending_batches:
                del self.pending_batches[connection_id]
            
            if connection_id in self.rate_limit_counters:
                del self.rate_limit_counters[connection_id]
            
            self.metrics.active_connections = len(self.connections)
            
            logger.debug(f"‚úÖ Connection unregistered: {connection_id}")
            
        except Exception as e:
            logger.error(f"‚ùå Error unregistering connection {connection_id}: {e}")
    
    async def send_message(self, connection_id: str, message: Dict[str, Any], 
                          use_batching: bool = True, compression: Optional[CompressionLevel] = None) -> bool:
        """Send optimized message to specific connection"""
        try:
            if connection_id not in self.connections:
                return False
            
            # Check rate limiting
            if not self._check_rate_limit(connection_id):
                logger.warning(f"Rate limit exceeded for connection: {connection_id}")
                return False
            
            start_time = time.time()
            
            if use_batching:
                # Add to batch
                self.pending_batches[connection_id].append(message)
                
                # Start batch timer if not already running
                if connection_id not in self.batch_timers:
                    self.batch_timers[connection_id] = asyncio.create_task(
                        self._batch_timer(connection_id)
                    )
                
                # Send immediately if batch is full
                if len(self.pending_batches[connection_id]) >= self.message_batch_size:
                    await self._send_batch(connection_id)
                
                return True
            else:
                # Send immediately
                return await self._send_single_message(connection_id, message, compression, start_time)
                
        except Exception as e:
            logger.error(f"‚ùå Error sending message to {connection_id}: {e}")
            self.metrics.error_count += 1
            return False
    
    async def broadcast_message(self, message: Dict[str, Any], group: str = "default", 
                              exclude: Optional[Set[str]] = None) -> int:
        """Broadcast optimized message to group of connections"""
        try:
            target_connections = self.connection_groups.get(group, set())
            if exclude:
                target_connections = target_connections - exclude
            
            if not target_connections:
                return 0
            
            # Optimize message for broadcasting
            optimized_message = await self._optimize_message_for_broadcast(message)
            
            # Send to all connections concurrently
            send_tasks = []
            for conn_id in target_connections:
                if conn_id in self.connections:
                    task = self._send_optimized_message(conn_id, optimized_message)
                    send_tasks.append(task)
            
            if send_tasks:
                results = await asyncio.gather(*send_tasks, return_exceptions=True)
                successful_sends = sum(1 for result in results if result is True)
                return successful_sends
            
            return 0
            
        except Exception as e:
            logger.error(f"‚ùå Error broadcasting message: {e}")
            self.metrics.error_count += 1
            return 0
    
    async def _send_single_message(self, connection_id: str, message: Dict[str, Any], 
                                 compression: Optional[CompressionLevel], start_time: float) -> bool:
        """Send single message with optimization"""
        try:
            websocket = self.connections[connection_id]
            
            # Serialize message
            message_data = json.dumps(message).encode('utf-8')
            
            # Apply compression if needed
            if compression or len(message_data) > self.compression_threshold:
                compression_level = compression or self.default_compression_level
                message_data = await self._compress_message(message_data, compression_level)
            
            # Send message
            await websocket.send(message_data)
            
            # Update metrics
            latency = (time.time() - start_time) * 1000  # Convert to milliseconds
            self._update_connection_stats(connection_id, len(message_data), latency, sent=True)
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error sending single message to {connection_id}: {e}")
            return False
    
    async def _send_batch(self, connection_id: str):
        """Send batched messages"""
        try:
            if connection_id not in self.pending_batches or not self.pending_batches[connection_id]:
                return
            
            batch = self.pending_batches[connection_id].copy()
            self.pending_batches[connection_id].clear()
            
            # Cancel timer
            if connection_id in self.batch_timers:
                self.batch_timers[connection_id].cancel()
                del self.batch_timers[connection_id]
            
            # Create batch message
            batch_message = {
                'type': 'batch',
                'messages': batch,
                'timestamp': datetime.now().isoformat()
            }
            
            await self._send_single_message(connection_id, batch_message, None, time.time())
            
        except Exception as e:
            logger.error(f"‚ùå Error sending batch to {connection_id}: {e}")
    
    async def _batch_timer(self, connection_id: str):
        """Timer for batch sending"""
        try:
            await asyncio.sleep(self.batch_timeout_ms / 1000.0)
            await self._send_batch(connection_id)
        except asyncio.CancelledError:
            pass
        except Exception as e:
            logger.error(f"‚ùå Error in batch timer for {connection_id}: {e}")
    
    async def _optimize_message_for_broadcast(self, message: Dict[str, Any]) -> bytes:
        """Optimize message for broadcasting"""
        try:
            # Create cache key
            cache_key = self._create_cache_key(message)
            
            # Check cache
            if cache_key in self.message_cache:
                self.metrics.cache_hit_rate = (self.metrics.cache_hit_rate * 0.9) + (1.0 * 0.1)
                return self.message_cache[cache_key]
            
            # Serialize and compress
            message_data = json.dumps(message).encode('utf-8')
            
            if len(message_data) > self.compression_threshold:
                message_data = await self._compress_message(message_data, self.default_compression_level)
            
            # Cache the result
            self.message_cache[cache_key] = message_data
            self.metrics.cache_hit_rate = (self.metrics.cache_hit_rate * 0.9) + (0.0 * 0.1)
            
            return message_data
            
        except Exception as e:
            logger.error(f"‚ùå Error optimizing message for broadcast: {e}")
            return json.dumps(message).encode('utf-8')
    
    async def _send_optimized_message(self, connection_id: str, message_data: bytes) -> bool:
        """Send pre-optimized message"""
        try:
            if connection_id not in self.connections:
                return False
            
            websocket = self.connections[connection_id]
            start_time = time.time()
            
            await websocket.send(message_data)
            
            # Update metrics
            latency = (time.time() - start_time) * 1000
            self._update_connection_stats(connection_id, len(message_data), latency, sent=True)
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error sending optimized message to {connection_id}: {e}")
            return False
    
    async def _compress_message(self, data: bytes, compression_level: CompressionLevel) -> bytes:
        """Compress message data"""
        try:
            loop = asyncio.get_event_loop()
            compressed_data = await loop.run_in_executor(
                self.thread_pool,
                lambda: gzip.compress(data, compresslevel=compression_level.value)
            )
            
            # Update compression ratio
            ratio = len(compressed_data) / len(data) if len(data) > 0 else 1.0
            self.metrics.compression_ratio = (self.metrics.compression_ratio * 0.9) + (ratio * 0.1)
            
            return compressed_data
            
        except Exception as e:
            logger.error(f"‚ùå Error compressing message: {e}")
            return data
    
    def _create_cache_key(self, message: Dict[str, Any]) -> str:
        """Create cache key for message"""
        try:
            # Remove timestamp and other volatile fields
            cache_message = message.copy()
            cache_message.pop('timestamp', None)
            cache_message.pop('id', None)
            
            return str(hash(json.dumps(cache_message, sort_keys=True)))
            
        except Exception as e:
            logger.error(f"‚ùå Error creating cache key: {e}")
            return str(time.time())
    
    def _check_rate_limit(self, connection_id: str) -> bool:
        """Check rate limiting for connection"""
        try:
            current_time = time.time()
            rate_counter = self.rate_limit_counters[connection_id]
            
            # Remove old entries
            while rate_counter and current_time - rate_counter[0] > self.rate_limit_window:
                rate_counter.popleft()
            
            # Check limit
            if len(rate_counter) >= self.rate_limit_per_connection:
                return False
            
            # Add current request
            rate_counter.append(current_time)
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Error checking rate limit for {connection_id}: {e}")
            return True  # Allow on error
    
    def _update_connection_stats(self, connection_id: str, bytes_count: int, 
                               latency: float, sent: bool = True):
        """Update connection statistics"""
        try:
            if connection_id not in self.connection_stats:
                return
            
            stats = self.connection_stats[connection_id]
            stats.last_activity = datetime.now()
            
            if sent:
                stats.messages_sent += 1
                stats.bytes_sent += bytes_count
                self.metrics.messages_sent += 1
                self.metrics.bytes_sent += bytes_count
            else:
                stats.messages_received += 1
                stats.bytes_received += bytes_count
                self.metrics.messages_received += 1
                self.metrics.bytes_received += bytes_received
            
            # Update latency
            stats.average_latency = (stats.average_latency * 0.9) + (latency * 0.1)
            self.latency_history.append(latency)
            
            # Update global metrics
            self.metrics.average_response_time = (self.metrics.average_response_time * 0.9) + (latency * 0.1)
            self.metrics.peak_response_time = max(self.metrics.peak_response_time, latency)
            
        except Exception as e:
            logger.error(f"‚ùå Error updating connection stats: {e}")
    
    async def _cleanup_loop(self):
        """Background cleanup loop"""
        while self.running:
            try:
                current_time = time.time()
                
                # Clean up message cache
                if current_time - self.last_cache_cleanup > self.cache_cleanup_interval:
                    self.message_cache.clear()
                    self.last_cache_cleanup = current_time
                    logger.debug("üßπ Message cache cleaned up")
                
                await asyncio.sleep(60)  # Run every minute
                
            except Exception as e:
                logger.error(f"‚ùå Error in cleanup loop: {e}")
                await asyncio.sleep(60)
    
    async def _metrics_loop(self):
        """Background metrics collection loop"""
        while self.running:
            try:
                self.metrics.last_update = datetime.now()
                
                # Calculate throughput
                if self.latency_history:
                    avg_latency = sum(self.latency_history) / len(self.latency_history)
                    self.throughput_history.append(1000.0 / avg_latency if avg_latency > 0 else 0)
                
                await asyncio.sleep(1)  # Update every second
                
            except Exception as e:
                logger.error(f"‚ùå Error in metrics loop: {e}")
                await asyncio.sleep(1)
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get current performance metrics"""
        try:
            current_latency = sum(self.latency_history) / len(self.latency_history) if self.latency_history else 0
            current_throughput = sum(self.throughput_history) / len(self.throughput_history) if self.throughput_history else 0
            
            return {
                'connections': {
                    'total': self.metrics.total_connections,
                    'active': self.metrics.active_connections,
                    'max_allowed': self.max_connections
                },
                'performance': {
                    'average_latency_ms': self.metrics.average_response_time,
                    'current_latency_ms': current_latency,
                    'peak_latency_ms': self.metrics.peak_response_time,
                    'target_latency_ms': self.target_latency_ms,
                    'throughput_msg_per_sec': current_throughput
                },
                'traffic': {
                    'messages_sent': self.metrics.messages_sent,
                    'messages_received': self.metrics.messages_received,
                    'bytes_sent': self.metrics.bytes_sent,
                    'bytes_received': self.metrics.bytes_received
                },
                'optimization': {
                    'compression_ratio': self.metrics.compression_ratio,
                    'cache_hit_rate': self.metrics.cache_hit_rate,
                    'cached_messages': len(self.message_cache)
                },
                'health': {
                    'error_count': self.metrics.error_count,
                    'last_update': self.metrics.last_update.isoformat() if self.metrics.last_update else None
                }
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error getting performance metrics: {e}")
            return {'error': str(e)}
