# Comprehensive Triple Straddle Engine V2.0 - Detailed Flow Documentation

## Complete Market Regime Formation Process - Step-by-Step Breakdown

**Date:** 2025-06-23  
**Version:** 2.0.0  
**Author:** The Augster  

---

## ðŸ“Š **Phase 1: Data Input & Validation**

### 1.1 Raw Market Data Input
```python
# Input data structure
market_data = {
    'spot_price': [18450.25, 18452.75, 18455.00, ...],  # Real-time spot prices
    'atm_ce_price': [125.50, 126.25, 127.00, ...],      # ATM Call prices
    'atm_pe_price': [128.75, 127.50, 126.25, ...],      # ATM Put prices
    'itm1_ce_price': [145.25, 146.00, 146.75, ...],     # ITM1 Call prices
    'itm1_pe_price': [108.50, 107.75, 107.00, ...],     # ITM1 Put prices
    'otm1_ce_price': [105.75, 106.50, 107.25, ...],     # OTM1 Call prices
    'otm1_pe_price': [148.25, 147.50, 146.75, ...],     # OTM1 Put prices
    'volumes': [1250, 1380, 1420, ...],                 # Trading volumes
    'timestamps': ['2025-06-23 09:15:00', ...]          # HeavyDB timestamps
}
```

### 1.2 Data Validation & Quality Check
```python
def validate_market_data(data):
    """Ensure 100% data completeness and mathematical precision"""
    validation_results = {
        'completeness': check_data_completeness(data),      # Target: 100%
        'precision': validate_mathematical_precision(data), # Target: Â±0.001
        'consistency': check_timestamp_consistency(data),   # Target: No gaps
        'quality_score': calculate_overall_quality(data)    # Target: >99%
    }
    return validation_results
```

---

## ðŸ“ˆ **Phase 2: Component Price Series Extraction**

### 2.1 Independent Component Calculation
```python
# Component price series (NO adjustment factors)
components = {
    'atm_straddle': atm_ce_price + atm_pe_price,           # Weight: 25%
    'itm1_straddle': itm1_ce_price + itm1_pe_price,       # Weight: 20%
    'otm1_straddle': otm1_ce_price + otm1_pe_price,       # Weight: 15%
    'combined_straddle': calculate_industry_standard_combination(),  # Weight: 20%
    'atm_ce': atm_ce_price,                                # Weight: 10%
    'atm_pe': atm_pe_price                                 # Weight: 10%
}
```

### 2.2 Industry-Standard Combined Straddle
```python
def calculate_combined_straddle(atm, itm1, otm1, dte=2, vix=18.5):
    """Industry-standard weighted combination with dynamic adjustments"""
    
    # Base weights
    base_weights = {'atm': 0.50, 'itm1': 0.30, 'otm1': 0.20}
    
    # DTE adjustments
    if dte <= 1:
        dte_adjustment = {'atm': 1.2, 'itm1': 0.9, 'otm1': 0.8}  # ATM emphasis
    elif dte <= 4:
        dte_adjustment = {'atm': 1.0, 'itm1': 1.0, 'otm1': 1.0}  # Balanced
    else:
        dte_adjustment = {'atm': 0.8, 'itm1': 1.1, 'otm1': 1.2}  # ITM/OTM emphasis
    
    # VIX adjustments
    if vix > 25:
        vix_adjustment = {'atm': 0.9, 'itm1': 1.0, 'otm1': 1.3}  # OTM emphasis
    elif vix < 15:
        vix_adjustment = {'atm': 1.2, 'itm1': 0.9, 'otm1': 0.8}  # ATM emphasis
    else:
        vix_adjustment = {'atm': 1.0, 'itm1': 1.0, 'otm1': 1.0}  # Neutral
    
    # Calculate adjusted weights
    adjusted_weights = {}
    for component in base_weights:
        adjusted_weights[component] = (
            base_weights[component] * 
            dte_adjustment[component] * 
            vix_adjustment[component]
        )
    
    # Renormalize to maintain 100% allocation
    total_weight = sum(adjusted_weights.values())
    for component in adjusted_weights:
        adjusted_weights[component] /= total_weight
    
    # Calculate combined straddle
    combined = (
        atm * adjusted_weights['atm'] +
        itm1 * adjusted_weights['itm1'] +
        otm1 * adjusted_weights['otm1']
    )
    
    return combined, adjusted_weights
```

---

## ðŸ”§ **Phase 3: Independent Technical Analysis**

### 3.1 EMA Analysis (Independent for Each Component)
```python
def calculate_independent_ema(component_prices):
    """Calculate EMA without any adjustment factors"""
    
    ema_results = {}
    
    # Independent EMA calculations
    ema_results['ema_20'] = component_prices.ewm(span=20, adjust=False).mean()
    ema_results['ema_100'] = component_prices.ewm(span=100, adjust=False).mean()
    ema_results['ema_200'] = component_prices.ewm(span=200, adjust=False).mean()
    
    # EMA alignment analysis
    current_price = component_prices.iloc[-1]
    ema_20_current = ema_results['ema_20'].iloc[-1]
    ema_100_current = ema_results['ema_100'].iloc[-1]
    ema_200_current = ema_results['ema_200'].iloc[-1]
    
    # Alignment scoring
    if ema_20_current > ema_100_current > ema_200_current:
        alignment = 'bullish'
        alignment_score = 1.0
    elif ema_20_current < ema_100_current < ema_200_current:
        alignment = 'bearish'
        alignment_score = -1.0
    else:
        alignment = 'mixed'
        # Calculate partial alignment score
        bullish_signals = sum([
            ema_20_current > ema_100_current,
            ema_100_current > ema_200_current,
            current_price > ema_20_current
        ])
        alignment_score = (bullish_signals / 3) * 2 - 1  # Scale to [-1, 1]
    
    ema_results['alignment'] = alignment
    ema_results['alignment_score'] = alignment_score
    
    return ema_results
```

### 3.2 VWAP Analysis (Independent for Each Component)
```python
def calculate_independent_vwap(component_prices, volumes):
    """Calculate VWAP without any adjustment factors"""
    
    vwap_results = {}
    
    # Current day VWAP calculation
    cumulative_volume = volumes.cumsum()
    cumulative_pv = (component_prices * volumes).cumsum()
    vwap_current = cumulative_pv / cumulative_volume
    
    # Previous day VWAP (using last 75 periods as proxy for previous day)
    if len(component_prices) >= 75:
        prev_day_end = len(component_prices) - 75
        prev_cumulative_volume = volumes.iloc[:prev_day_end].cumsum()
        prev_cumulative_pv = (component_prices.iloc[:prev_day_end] * volumes.iloc[:prev_day_end]).cumsum()
        vwap_previous = prev_cumulative_pv.iloc[-1] / prev_cumulative_volume.iloc[-1]
    else:
        vwap_previous = vwap_current.iloc[0]  # Fallback to first VWAP value
    
    # VWAP position analysis
    current_price = component_prices.iloc[-1]
    vwap_current_value = vwap_current.iloc[-1]
    
    vwap_position = (current_price / vwap_current_value - 1) * 100  # Percentage above/below
    
    if current_price > vwap_current_value:
        vwap_bias = 'bullish'
        vwap_score = min(abs(vwap_position) / 2, 1.0)  # Cap at 1.0
    elif current_price < vwap_current_value:
        vwap_bias = 'bearish'
        vwap_score = -min(abs(vwap_position) / 2, 1.0)  # Cap at -1.0
    else:
        vwap_bias = 'neutral'
        vwap_score = 0.0
    
    vwap_results['vwap_current'] = vwap_current
    vwap_results['vwap_previous'] = vwap_previous
    vwap_results['vwap_position'] = vwap_position
    vwap_results['vwap_bias'] = vwap_bias
    vwap_results['vwap_score'] = vwap_score
    
    return vwap_results
```

### 3.3 Pivot Point Analysis (Independent for Each Component)
```python
def calculate_independent_pivots(component_prices):
    """Calculate pivot points without any adjustment factors"""
    
    pivot_results = {}
    
    # Use rolling 75-period window to simulate daily high/low/close
    window_size = 75
    
    if len(component_prices) >= window_size:
        rolling_high = component_prices.rolling(window=window_size).max()
        rolling_low = component_prices.rolling(window=window_size).min()
        rolling_close = component_prices  # Current price as close
        
        # Pivot point calculation
        pivot_current = (rolling_high + rolling_low + rolling_close) / 3
        
        # Support and resistance levels
        support_1 = 2 * pivot_current - rolling_high
        support_2 = pivot_current - (rolling_high - rolling_low)
        resistance_1 = 2 * pivot_current - rolling_low
        resistance_2 = pivot_current + (rolling_high - rolling_low)
        
        # Current position analysis
        current_price = component_prices.iloc[-1]
        pivot_current_value = pivot_current.iloc[-1]
        
        if current_price > pivot_current_value:
            pivot_bias = 'bullish'
            pivot_score = min((current_price / pivot_current_value - 1) * 5, 1.0)
        elif current_price < pivot_current_value:
            pivot_bias = 'bearish'
            pivot_score = -min((pivot_current_value / current_price - 1) * 5, 1.0)
        else:
            pivot_bias = 'neutral'
            pivot_score = 0.0
        
        pivot_results['pivot_current'] = pivot_current
        pivot_results['support_1'] = support_1
        pivot_results['support_2'] = support_2
        pivot_results['resistance_1'] = resistance_1
        pivot_results['resistance_2'] = resistance_2
        pivot_results['pivot_bias'] = pivot_bias
        pivot_results['pivot_score'] = pivot_score
        
        # Extract S&R levels for confluence analysis
        pivot_results['sr_levels'] = {
            'pivot': pivot_current_value,
            's1': support_1.iloc[-1],
            's2': support_2.iloc[-1],
            'r1': resistance_1.iloc[-1],
            'r2': resistance_2.iloc[-1]
        }
    else:
        # Insufficient data fallback
        pivot_results = {
            'pivot_bias': 'neutral',
            'pivot_score': 0.0,
            'sr_levels': {}
        }
    
    return pivot_results
```

---

## â±ï¸ **Phase 4: Multi-Timeframe Rolling Analysis**

### 4.1 Timeframe Configuration
```python
timeframe_config = {
    '3min': {
        'periods': 3,
        'weight': 0.15,
        'description': 'Short-term momentum capture',
        'window_size': 3
    },
    '5min': {
        'periods': 5,
        'weight': 0.25,
        'description': 'Primary analysis timeframe',
        'window_size': 5
    },
    '10min': {
        'periods': 10,
        'weight': 0.30,
        'description': 'Medium-term structure analysis',
        'window_size': 10
    },
    '15min': {
        'periods': 15,
        'weight': 0.30,
        'description': 'Long-term validation',
        'window_size': 15
    }
}
```

### 4.2 Multi-Timeframe Technical Analysis Application
```python
def apply_multi_timeframe_analysis(component_prices, volumes, timeframes):
    """Apply technical analysis across multiple timeframes independently"""
    
    timeframe_results = {}
    
    for timeframe, config in timeframes.items():
        window = config['periods']
        weight = config['weight']
        
        # Apply rolling window to simulate timeframe
        if len(component_prices) >= window:
            # Resample data to timeframe (simplified approach)
            tf_prices = component_prices.rolling(window=window).mean()
            tf_volumes = volumes.rolling(window=window).sum()
            
            # Apply independent technical analysis
            ema_results = calculate_independent_ema(tf_prices)
            vwap_results = calculate_independent_vwap(tf_prices, tf_volumes)
            pivot_results = calculate_independent_pivots(tf_prices)
            
            # Combine results for this timeframe
            timeframe_results[timeframe] = {
                'ema': ema_results,
                'vwap': vwap_results,
                'pivot': pivot_results,
                'weight': weight,
                'technical_score': calculate_timeframe_technical_score(
                    ema_results, vwap_results, pivot_results
                )
            }
        else:
            # Insufficient data fallback
            timeframe_results[timeframe] = {
                'technical_score': 0.0,
                'weight': weight,
                'status': 'insufficient_data'
            }
    
    return timeframe_results

def calculate_timeframe_technical_score(ema_results, vwap_results, pivot_results):
    """Calculate combined technical score for a timeframe"""
    
    # Weight the three technical indicators equally
    indicator_weights = {'ema': 0.4, 'vwap': 0.35, 'pivot': 0.25}
    
    technical_score = (
        ema_results['alignment_score'] * indicator_weights['ema'] +
        vwap_results['vwap_score'] * indicator_weights['vwap'] +
        pivot_results['pivot_score'] * indicator_weights['pivot']
    )
    
    return technical_score
```

---

## ðŸ”„ **Phase 5: 6Ã—6 Rolling Correlation Matrix Analysis**

### 5.1 Correlation Matrix Construction
```python
def construct_correlation_matrix(component_results, correlation_windows=[20, 50, 100]):
    """Build comprehensive 6Ã—6 rolling correlation matrix"""
    
    correlation_results = {}
    
    # Extract price series for all components
    component_prices = {}
    for component, results in component_results.items():
        component_prices[component] = results['price_series']
    
    # Component-to-Component Correlations (15 pairs)
    component_correlations = {}
    components = list(component_prices.keys())
    
    for i, comp1 in enumerate(components):
        for j, comp2 in enumerate(components[i+1:], i+1):
            pair_name = f"{comp1}_vs_{comp2}"
            
            # Calculate rolling correlations for different windows
            correlations = {}
            for window in correlation_windows:
                if len(component_prices[comp1]) >= window:
                    rolling_corr = component_prices[comp1].rolling(window).corr(
                        component_prices[comp2]
                    )
                    correlations[f'window_{window}'] = {
                        'current': rolling_corr.iloc[-1],
                        'mean': rolling_corr.mean(),
                        'std': rolling_corr.std(),
                        'stability': 1 - rolling_corr.std()  # Higher = more stable
                    }
                else:
                    correlations[f'window_{window}'] = {
                        'current': 0.0,
                        'mean': 0.0,
                        'std': 1.0,
                        'stability': 0.0
                    }
            
            component_correlations[pair_name] = correlations
    
    correlation_results['component_correlations'] = component_correlations
    
    return correlation_results
```

### 5.2 Correlation Classification and Scoring
```python
def classify_and_score_correlations(correlation_results):
    """Classify correlations and calculate regime coherence score"""
    
    classification_thresholds = {
        'high_positive': 0.8,
        'medium_positive': 0.4,
        'low_positive': 0.1,
        'negative': -0.1
    }
    
    classified_correlations = {}
    coherence_metrics = {
        'high_correlations': 0,
        'medium_correlations': 0,
        'low_correlations': 0,
        'negative_correlations': 0,
        'total_correlations': 0
    }
    
    # Classify component correlations
    for pair_name, correlations in correlation_results['component_correlations'].items():
        # Use 50-period window as primary
        primary_corr = correlations['window_50']['current']
        stability = correlations['window_50']['stability']
        
        # Classify correlation
        if primary_corr > classification_thresholds['high_positive']:
            classification = 'high_positive'
            coherence_metrics['high_correlations'] += 1
        elif primary_corr > classification_thresholds['medium_positive']:
            classification = 'medium_positive'
            coherence_metrics['medium_correlations'] += 1
        elif primary_corr > classification_thresholds['low_positive']:
            classification = 'low_positive'
            coherence_metrics['low_correlations'] += 1
        else:
            classification = 'negative'
            coherence_metrics['negative_correlations'] += 1
        
        coherence_metrics['total_correlations'] += 1
        
        classified_correlations[pair_name] = {
            'correlation': primary_corr,
            'classification': classification,
            'stability': stability,
            'confidence': calculate_correlation_confidence(primary_corr, stability)
        }
    
    # Calculate regime coherence
    total_corr = coherence_metrics['total_correlations']
    if total_corr > 0:
        regime_coherence = (
            coherence_metrics['high_correlations'] * 1.0 +
            coherence_metrics['medium_correlations'] * 0.6 +
            coherence_metrics['low_correlations'] * 0.3 +
            coherence_metrics['negative_correlations'] * 0.0
        ) / total_corr
    else:
        regime_coherence = 0.0
    
    return {
        'classified_correlations': classified_correlations,
        'coherence_metrics': coherence_metrics,
        'regime_coherence': regime_coherence,
        'correlation_confidence': calculate_overall_correlation_confidence(classified_correlations)
    }

def calculate_correlation_confidence(correlation, stability):
    """Calculate confidence score for individual correlation"""
    
    # Base confidence from correlation strength
    base_confidence = min(abs(correlation), 1.0)
    
    # Stability bonus (stable correlations are more reliable)
    stability_bonus = stability * 0.2
    
    # Final confidence
    confidence = min(base_confidence + stability_bonus, 1.0)
    
    return confidence
```

---

## ðŸŽ¯ **Phase 6: Support & Resistance Confluence Analysis**

### 6.1 Cross-Component S&R Level Extraction
```python
def extract_sr_levels_from_components(component_technical_results):
    """Extract S&R levels from all component technical analysis"""

    all_sr_levels = []

    for component, timeframe_results in component_technical_results.items():
        for timeframe, technical_data in timeframe_results.items():
            if 'sr_levels' in technical_data:
                sr_levels = technical_data['sr_levels']

                for level_type, level_value in sr_levels.items():
                    all_sr_levels.append({
                        'component': component,
                        'timeframe': timeframe,
                        'level_type': level_type,  # pivot, s1, s2, r1, r2, vwap, ema_20, etc.
                        'level_value': level_value,
                        'source': 'technical_analysis'
                    })

    return all_sr_levels
```

### 6.2 Confluence Zone Detection (0.5% Tolerance)
```python
def detect_confluence_zones(all_sr_levels, tolerance=0.005):
    """Detect confluence zones with 0.5% tolerance"""

    confluence_zones = []
    processed_levels = set()

    for i, level in enumerate(all_sr_levels):
        if i in processed_levels:
            continue

        confluence_group = [level]
        level_value = level['level_value']

        # Find all levels within tolerance
        for j, other_level in enumerate(all_sr_levels[i+1:], i+1):
            if j in processed_levels:
                continue

            other_value = other_level['level_value']
            relative_diff = abs(other_value - level_value) / level_value

            if relative_diff <= tolerance:
                confluence_group.append(other_level)
                processed_levels.add(j)

        # Only create confluence zone if multiple levels converge
        if len(confluence_group) >= 2:
            zone_center = np.mean([l['level_value'] for l in confluence_group])
            zone_range = max([l['level_value'] for l in confluence_group]) - min([l['level_value'] for l in confluence_group])

            # Count unique components and timeframes
            unique_components = len(set([l['component'] for l in confluence_group]))
            unique_timeframes = len(set([l['timeframe'] for l in confluence_group]))

            confluence_zone = {
                'center': zone_center,
                'range': zone_range,
                'strength': len(confluence_group),
                'levels': confluence_group,
                'components_involved': unique_components,
                'timeframes_involved': unique_timeframes,
                'zone_type': classify_zone_type(confluence_group)
            }

            confluence_zones.append(confluence_zone)
            processed_levels.add(i)

    return confluence_zones

def classify_zone_type(confluence_group):
    """Classify confluence zone as support, resistance, or pivot"""

    level_types = [level['level_type'] for level in confluence_group]

    support_types = ['s1', 's2', 'support', 'vwap_support']
    resistance_types = ['r1', 'r2', 'resistance', 'vwap_resistance']
    pivot_types = ['pivot', 'vwap', 'ema_20', 'ema_100', 'ema_200']

    support_count = sum(1 for lt in level_types if any(st in lt for st in support_types))
    resistance_count = sum(1 for lt in level_types if any(rt in lt for rt in resistance_types))
    pivot_count = sum(1 for lt in level_types if any(pt in lt for pt in pivot_types))

    if support_count > resistance_count and support_count > pivot_count:
        return 'support'
    elif resistance_count > support_count and resistance_count > pivot_count:
        return 'resistance'
    else:
        return 'pivot'
```

### 6.3 Dynamic S&R Strength Scoring
```python
def calculate_sr_strength_score(confluence_zones, current_price):
    """Calculate dynamic strength score for S&R analysis"""

    if not confluence_zones:
        return {
            'sr_score': 0.0,
            'zone_count': 0,
            'strongest_zone': None,
            'breakout_signals': []
        }

    zone_scores = []
    breakout_signals = []

    for zone in confluence_zones:
        # Base strength from number of converging levels
        base_strength = min(zone['strength'] / 10, 1.0)

        # Cross-component involvement bonus
        component_bonus = min(zone['components_involved'] / 6, 1.0) * 0.2

        # Timeframe involvement bonus
        timeframe_bonus = min(zone['timeframes_involved'] / 4, 1.0) * 0.15

        # Static + Dynamic mix bonus
        level_types = [level['level_type'] for level in zone['levels']]
        has_static = any('pivot' in lt or 'vwap' in lt for lt in level_types)
        has_dynamic = any('ema' in lt for lt in level_types)
        mix_bonus = 0.1 if (has_static and has_dynamic) else 0

        # Distance from current price (closer zones are more relevant)
        distance_ratio = abs(current_price - zone['center']) / current_price
        distance_factor = max(1 - distance_ratio * 10, 0.1)  # Closer = higher factor

        # Final zone strength
        zone_strength = (base_strength + component_bonus + timeframe_bonus + mix_bonus) * distance_factor
        zone['calculated_strength'] = zone_strength
        zone_scores.append(zone_strength)

        # Check for breakouts/breakdowns
        zone_upper = zone['center'] + zone['range'] / 2
        zone_lower = zone['center'] - zone['range'] / 2

        if current_price > zone_upper and zone['zone_type'] in ['resistance', 'pivot']:
            breakout_signals.append({
                'type': 'breakout',
                'zone': zone,
                'strength': zone_strength,
                'signal_strength': (current_price - zone_upper) / zone_upper
            })
        elif current_price < zone_lower and zone['zone_type'] in ['support', 'pivot']:
            breakout_signals.append({
                'type': 'breakdown',
                'zone': zone,
                'strength': zone_strength,
                'signal_strength': (zone_lower - current_price) / zone_lower
            })

    # Overall S&R score
    if zone_scores:
        sr_score = np.mean(zone_scores)
        strongest_zone = max(confluence_zones, key=lambda z: z['calculated_strength'])
    else:
        sr_score = 0.0
        strongest_zone = None

    return {
        'sr_score': sr_score,
        'zone_count': len(confluence_zones),
        'strongest_zone': strongest_zone,
        'breakout_signals': breakout_signals,
        'confluence_zones': confluence_zones
    }
```

---

## âš–ï¸ **Phase 7: Weighted Regime Score Calculation**

### 7.1 Master Scoring Formula
```python
def calculate_weighted_regime_score(analysis_results):
    """Calculate final weighted regime score from all components"""

    # Master scoring weights
    scoring_weights = {
        'correlation_analysis': 0.30,      # 30% - Correlation matrix analysis
        'technical_alignment': 0.25,       # 25% - Technical indicator consensus
        'sr_confluence': 0.20,             # 20% - Support/Resistance analysis
        'component_consensus': 0.15,       # 15% - Cross-component agreement
        'timeframe_consistency': 0.10      # 10% - Multi-timeframe alignment
    }

    # Extract individual scores
    correlation_score = analysis_results.get('correlation_analysis', {}).get('regime_coherence', 0.0)
    technical_score = analysis_results.get('technical_alignment', {}).get('alignment_score', 0.0)
    sr_score = analysis_results.get('sr_analysis', {}).get('sr_score', 0.0)
    consensus_score = analysis_results.get('component_consensus', {}).get('consensus_score', 0.0)
    consistency_score = analysis_results.get('timeframe_consistency', {}).get('consistency_score', 0.0)

    # Calculate weighted final score
    final_score = (
        correlation_score * scoring_weights['correlation_analysis'] +
        technical_score * scoring_weights['technical_alignment'] +
        sr_score * scoring_weights['sr_confluence'] +
        consensus_score * scoring_weights['component_consensus'] +
        consistency_score * scoring_weights['timeframe_consistency']
    )

    # Calculate signal direction and strength
    signal_direction = calculate_signal_direction(analysis_results)
    signal_strength = calculate_signal_strength(analysis_results)

    return {
        'final_score': final_score,
        'signal_direction': signal_direction,
        'signal_strength': signal_strength,
        'component_breakdown': {
            'correlation': correlation_score,
            'technical': technical_score,
            'sr': sr_score,
            'consensus': consensus_score,
            'consistency': consistency_score
        },
        'weights_used': scoring_weights
    }

def calculate_signal_direction(analysis_results):
    """Calculate overall signal direction from all components"""

    directional_signals = []

    # Technical alignment direction
    tech_score = analysis_results.get('technical_alignment', {}).get('alignment_score', 0.0)
    directional_signals.append(tech_score)

    # Component consensus direction
    consensus_data = analysis_results.get('component_consensus', {})
    if 'bullish_ratio' in consensus_data and 'bearish_ratio' in consensus_data:
        consensus_direction = consensus_data['bullish_ratio'] - consensus_data['bearish_ratio']
        directional_signals.append(consensus_direction)

    # S&R breakout direction
    sr_data = analysis_results.get('sr_analysis', {})
    breakout_signals = sr_data.get('breakout_signals', [])
    if breakout_signals:
        breakout_direction = sum(
            1 if signal['type'] == 'breakout' else -1
            for signal in breakout_signals
        ) / len(breakout_signals)
        directional_signals.append(breakout_direction)

    # Calculate weighted average direction
    if directional_signals:
        signal_direction = np.mean(directional_signals)
    else:
        signal_direction = 0.0

    return signal_direction

def calculate_signal_strength(analysis_results):
    """Calculate overall signal strength from all components"""

    strength_indicators = []

    # Correlation strength
    corr_data = analysis_results.get('correlation_analysis', {})
    if 'coherence_metrics' in corr_data:
        high_corr_ratio = corr_data['coherence_metrics'].get('high_correlations', 0) / max(
            corr_data['coherence_metrics'].get('total_correlations', 1), 1
        )
        strength_indicators.append(high_corr_ratio)

    # Technical alignment strength
    tech_score = abs(analysis_results.get('technical_alignment', {}).get('alignment_score', 0.0))
    strength_indicators.append(tech_score)

    # S&R confluence strength
    sr_score = analysis_results.get('sr_analysis', {}).get('sr_score', 0.0)
    strength_indicators.append(sr_score)

    # Component consensus strength
    consensus_score = analysis_results.get('component_consensus', {}).get('consensus_score', 0.0)
    strength_indicators.append(consensus_score)

    # Calculate overall strength
    if strength_indicators:
        signal_strength = np.mean(strength_indicators)
    else:
        signal_strength = 0.0

    return signal_strength
```

---

## ðŸŽ¯ **Phase 8: Regime Classification & Confidence Calculation**

### 8.1 15-Regime Classification Logic
```python
def classify_market_regime(weighted_score_results):
    """Classify market regime based on weighted score and signal characteristics"""

    final_score = weighted_score_results['final_score']
    signal_direction = weighted_score_results['signal_direction']
    signal_strength = weighted_score_results['signal_strength']

    # Regime classification logic
    if final_score >= 0.8:
        if signal_direction > 0.2:
            regime_type = 1
            regime_name = "Strong_Bullish_Momentum"
        elif signal_direction < -0.2:
            regime_type = 11
            regime_name = "Strong_Bearish_Momentum"
        else:
            regime_type = 12
            regime_name = "High_Volatility_Regime"

    elif final_score >= 0.6:
        if signal_direction > 0.1:
            regime_type = 2
            regime_name = "Moderate_Bullish_Trend"
        elif signal_direction < -0.1:
            regime_type = 10
            regime_name = "Moderate_Bearish_Trend"
        else:
            regime_type = 6 if signal_strength > 0.3 else 7
            regime_name = "Neutral_Volatile" if signal_strength > 0.3 else "Neutral_Low_Volatility"

    elif final_score >= 0.4:
        if signal_direction > 0.05:
            if signal_strength < 0.3:
                regime_type = 4
                regime_name = "Bullish_Consolidation"
            else:
                regime_type = 3
                regime_name = "Weak_Bullish_Bias"
        elif signal_direction < -0.05:
            if signal_strength < 0.3:
                regime_type = 8
                regime_name = "Bearish_Consolidation"
            else:
                regime_type = 9
                regime_name = "Weak_Bearish_Bias"
        else:
            regime_type = 5
            regime_name = "Neutral_Balanced"

    elif final_score >= 0.2:
        if signal_strength > 0.4:
            regime_type = 6
            regime_name = "Neutral_Volatile"
        else:
            regime_type = 7
            regime_name = "Neutral_Low_Volatility"

    else:
        # Special condition checks
        if signal_strength > 0.6:
            regime_type = 12
            regime_name = "High_Volatility_Regime"
        elif signal_strength < 0.1:
            regime_type = 13
            regime_name = "Low_Volatility_Regime"
        elif abs(signal_direction) < 0.05 and 0.1 < signal_strength < 0.3:
            regime_type = 14
            regime_name = "Transition_Regime"
        else:
            regime_type = 15
            regime_name = "Undefined_Regime"

    return {
        'regime_type': regime_type,
        'regime_name': regime_name,
        'classification_confidence': calculate_classification_confidence(
            final_score, signal_direction, signal_strength
        )
    }

def calculate_classification_confidence(final_score, signal_direction, signal_strength):
    """Calculate confidence in regime classification"""

    # Base confidence from final score
    base_confidence = final_score

    # Direction clarity bonus
    direction_clarity = min(abs(signal_direction) * 2, 0.2)

    # Signal strength bonus
    strength_bonus = min(signal_strength * 0.15, 0.15)

    # Consistency bonus (if score and direction align)
    if (final_score > 0.5 and abs(signal_direction) > 0.1) or (final_score <= 0.5 and abs(signal_direction) <= 0.1):
        consistency_bonus = 0.1
    else:
        consistency_bonus = 0.0

    # Final confidence calculation
    classification_confidence = min(
        base_confidence + direction_clarity + strength_bonus + consistency_bonus,
        1.0
    )

    return classification_confidence
```

### 8.2 Enhanced Confidence Level Calculation
```python
def calculate_enhanced_confidence_level(regime_classification, analysis_results):
    """Calculate enhanced confidence level with multiple validation layers"""

    base_confidence = regime_classification['classification_confidence']

    # Component consistency boost
    component_scores = analysis_results.get('component_breakdown', {})
    score_values = list(component_scores.values())
    if score_values:
        score_std = np.std(score_values)
        consistency_boost = max(0, (1 - score_std * 2)) * 0.15  # Lower std = higher boost
    else:
        consistency_boost = 0.0

    # Correlation stability boost
    corr_data = analysis_results.get('correlation_analysis', {})
    if 'correlation_confidence' in corr_data:
        correlation_boost = corr_data['correlation_confidence'] * 0.1
    else:
        correlation_boost = 0.0

    # S&R confluence boost
    sr_data = analysis_results.get('sr_analysis', {})
    if sr_data.get('zone_count', 0) > 0:
        sr_boost = min(sr_data['zone_count'] / 10, 0.1)
    else:
        sr_boost = 0.0

    # Timeframe alignment boost
    consistency_data = analysis_results.get('timeframe_consistency', {})
    if 'consistency_score' in consistency_data:
        timeframe_boost = consistency_data['consistency_score'] * 0.05
    else:
        timeframe_boost = 0.0

    # Final enhanced confidence
    enhanced_confidence = min(
        base_confidence + consistency_boost + correlation_boost + sr_boost + timeframe_boost,
        1.0
    )

    return {
        'enhanced_confidence': enhanced_confidence,
        'confidence_breakdown': {
            'base_confidence': base_confidence,
            'consistency_boost': consistency_boost,
            'correlation_boost': correlation_boost,
            'sr_boost': sr_boost,
            'timeframe_boost': timeframe_boost
        },
        'confidence_level': classify_confidence_level(enhanced_confidence)
    }

def classify_confidence_level(confidence):
    """Classify confidence into descriptive levels"""

    if confidence >= 0.9:
        return "Very High"
    elif confidence >= 0.75:
        return "High"
    elif confidence >= 0.6:
        return "Medium"
    elif confidence >= 0.45:
        return "Low"
    else:
        return "Very Low"
```

---

## ðŸ“Š **Phase 9: Performance Validation & Output Generation**

### 9.1 Performance Metrics Tracking
```python
def track_performance_metrics(start_time, analysis_results, regime_output):
    """Track and validate performance metrics"""

    end_time = datetime.now()
    total_processing_time = (end_time - start_time).total_seconds()

    performance_metrics = {
        'processing_time': {
            'total_seconds': total_processing_time,
            'target_seconds': 3.0,
            'target_achieved': total_processing_time < 3.0,
            'efficiency_ratio': 3.0 / max(total_processing_time, 0.1)
        },
        'accuracy_metrics': {
            'regime_confidence': regime_output['enhanced_confidence'],
            'target_confidence': 0.90,
            'target_achieved': regime_output['enhanced_confidence'] > 0.90,
            'confidence_level': regime_output['confidence_level']
        },
        'data_quality': {
            'completeness': analysis_results.get('data_completeness', 0.0),
            'mathematical_precision': analysis_results.get('mathematical_precision', 0.0),
            'target_completeness': 1.0,
            'target_precision': 0.999
        },
        'system_health': {
            'components_analyzed': len(analysis_results.get('component_breakdown', {})),
            'correlations_calculated': analysis_results.get('correlation_analysis', {}).get('coherence_metrics', {}).get('total_correlations', 0),
            'sr_zones_detected': analysis_results.get('sr_analysis', {}).get('zone_count', 0),
            'timeframes_processed': 4,
            'overall_health': calculate_system_health_score(analysis_results)
        }
    }

    return performance_metrics

def calculate_system_health_score(analysis_results):
    """Calculate overall system health score"""

    health_indicators = []

    # Component coverage
    component_count = len(analysis_results.get('component_breakdown', {}))
    component_health = min(component_count / 5, 1.0)  # Target: 5 components
    health_indicators.append(component_health)

    # Correlation matrix health
    corr_data = analysis_results.get('correlation_analysis', {})
    if 'coherence_metrics' in corr_data:
        total_corr = corr_data['coherence_metrics'].get('total_correlations', 0)
        correlation_health = min(total_corr / 15, 1.0)  # Target: 15 correlations
        health_indicators.append(correlation_health)

    # S&R analysis health
    sr_data = analysis_results.get('sr_analysis', {})
    sr_health = 1.0 if sr_data.get('zone_count', 0) > 0 else 0.5
    health_indicators.append(sr_health)

    # Technical analysis health
    tech_data = analysis_results.get('technical_alignment', {})
    tech_health = 1.0 if 'alignment_score' in tech_data else 0.0
    health_indicators.append(tech_health)

    # Overall health score
    if health_indicators:
        overall_health = np.mean(health_indicators)
    else:
        overall_health = 0.0

    return overall_health
```

### 9.2 Final Output Generation
```python
def generate_final_regime_output(regime_classification, confidence_results,
                               analysis_results, performance_metrics):
    """Generate comprehensive final output"""

    final_output = {
        # Core regime information
        'regime_type': regime_classification['regime_type'],
        'regime_name': regime_classification['regime_name'],
        'regime_confidence': confidence_results['enhanced_confidence'],
        'confidence_level': confidence_results['confidence_level'],

        # Detailed scoring breakdown
        'scoring_breakdown': {
            'final_score': analysis_results.get('final_score', 0.0),
            'signal_direction': analysis_results.get('signal_direction', 0.0),
            'signal_strength': analysis_results.get('signal_strength', 0.0),
            'component_scores': analysis_results.get('component_breakdown', {}),
            'weights_applied': analysis_results.get('weights_used', {})
        },

        # Component analysis details
        'component_analysis': {
            'correlation_analysis': analysis_results.get('correlation_analysis', {}),
            'technical_alignment': analysis_results.get('technical_alignment', {}),
            'sr_confluence': analysis_results.get('sr_analysis', {}),
            'component_consensus': analysis_results.get('component_consensus', {}),
            'timeframe_consistency': analysis_results.get('timeframe_consistency', {})
        },

        # Confidence breakdown
        'confidence_analysis': confidence_results['confidence_breakdown'],

        # Performance metrics
        'performance_metrics': performance_metrics,

        # System metadata
        'metadata': {
            'timestamp': datetime.now().isoformat(),
            'version': '2.0.0',
            'architecture': 'comprehensive_rolling_based',
            'processing_mode': 'independent_technical_analysis',
            'correlation_matrix': '6x6_rolling',
            'regime_classification': '15_type_system'
        },

        # Quality assurance
        'quality_assurance': {
            'data_completeness': analysis_results.get('data_completeness', 0.0),
            'mathematical_accuracy': analysis_results.get('mathematical_precision', 0.0),
            'system_health': performance_metrics['system_health']['overall_health'],
            'validation_passed': validate_output_quality(analysis_results, performance_metrics)
        }
    }

    return final_output

def validate_output_quality(analysis_results, performance_metrics):
    """Validate output meets quality standards"""

    quality_checks = {
        'processing_time': performance_metrics['processing_time']['target_achieved'],
        'data_completeness': analysis_results.get('data_completeness', 0.0) > 0.99,
        'mathematical_precision': analysis_results.get('mathematical_precision', 0.0) > 0.999,
        'component_coverage': len(analysis_results.get('component_breakdown', {})) >= 5,
        'correlation_coverage': analysis_results.get('correlation_analysis', {}).get('coherence_metrics', {}).get('total_correlations', 0) >= 10
    }

    # All quality checks must pass
    validation_passed = all(quality_checks.values())

    return validation_passed
```

This comprehensive documentation provides complete mathematical formulas, step-by-step processes, and detailed implementation guidance for the entire Market Regime Formation Process in the Comprehensive Triple Straddle Engine V2.0.
